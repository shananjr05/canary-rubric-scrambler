Problem Statement

Automated tools that claim to detect AI-generated text are increasingly used in academic settings. However, existing detection methods suffer from high false-positive rates, lack of transparency, and poor robustness to paraphrasing or mixed human–AI authorship.

At the same time, generative AI systems often fail in tasks that require local context, instructional grounding, or reflective justification tied to specific learning environments. These limitations are not well captured by statistical detectors.

This project explores whether assessment design techniques, referred to here as cognitive canaries, can more reliably surface over-reliance on generative AI than automated detection tools. Rather than classifying submissions as “AI” or “human,” the focus is on identifying failure modes where AI-generated or AI-heavy responses struggle to meet context-sensitive academic constraints.

The goal is not enforcement or punishment, but to evaluate the limits of current AI detection approaches and propose safer, more transparent alternatives grounded in instructional design.
