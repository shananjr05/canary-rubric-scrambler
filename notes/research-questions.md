Research Questions:

How reliable are automated AI-detection tools when evaluating submissions that mix human and AI-generated content?

Can context-dependent assessment constraints (“cognitive canaries”) reveal failure modes in generative AI outputs more clearly than statistical detection methods?

What trade-offs exist between automated detection and human-in-the-loop evaluation approaches in academic assessment?

How do cognitive canaries affect false-positive and false-negative rates compared to existing AI-detection tools?